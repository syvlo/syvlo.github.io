<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>        
    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Sylvain  Lobry | Publications</title>
    <meta name="author" content="Sylvain  Lobry" />
    <meta name="description" content="Sylvain Lobry's academic website.
" />
    <meta name="keywords" content="Remote Sensing, Computer Vision, Image Processing, Universit√©, Ma√Ætre de conf√©rences" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

    <!-- Styles -->
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üõ∞</text></svg>">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://www.sylvainlobry.com/publications/">

    <!-- Dark Mode -->
    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
  </head>

  <!-- Body -->
  <body class="fixed-top-nav">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://www.sylvainlobry.com/"><span class="font-weight-bold">Sylvain</span>   Lobry</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">Projects</a>
              </li>
              <li class="nav-item active">
                <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/research/">Research activities</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/students/">Students</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">Teaching</a>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">Publications</h1>
            <p class="post-description"></p>
          </header>

          <article>
            <!-- _pages/publications.md -->
<div class="publications">

<h2>International Journals</h2>
  <h2 class="year">2021</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="fatras2021wasserstein" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Wasserstein Adversarial Regularization for learning with label noise</div>
          <!-- Author -->
          <div class="author">Fatras, Kilian,¬†Damodaran, Bharath Bhushan,¬†
                  <em>Lobry, Sylvain</em>,¬†Flamary, Remi,¬†Tuia, Devis,¬†and Courty, Nicolas
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="http://arxiv.org/abs/1904.03936" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/9477020" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Library</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Noisy labels often occur in vision datasets, especially when they are obtained from crowdsourcing or Web scraping. We propose a new regularization method, which enables learning robust classifiers in presence of noisy data. To achieve this goal, we propose a new adversarial regularization scheme based on the Wasserstein distance. Using this distance allows taking into account specific relations between classes by leveraging the geometric properties of the labels space. Our Wasserstein Adversarial Regularization (WAR) encodes a selective regularization, which promotes smoothness of the classifier between some classes, while preserving sufficient complexity of the decision boundary between others. We first discuss how and why adversarial regularization can be used in the context of noise and then show the effectiveness of our method on five datasets corrupted with noisy labels: in both benchmarks and real datasets, WAR outperforms the state-of-the-art competitors.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">fatras2021wasserstein</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Wasserstein Adversarial Regularization for learning with label noise}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Fatras, Kilian and Damodaran, Bharath Bhushan and Lobry, Sylvain and Flamary, Remi and Tuia, Devis and Courty, Nicolas}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Pattern Analysis and Machine Intelligence}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/TPAMI.2021.3094662}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/abstract/document/9477020}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{1904.03936}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="MAGNUSSON2021146877" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Shrub decline and expansion of wetland vegetation revealed by very high resolution land cover change detection in the Siberian lowland tundra</div>
          <!-- Author -->
          <div class="author">Magn√∫sson, R√∫na √ç.,¬†Limpens, Juul,¬†Kleijn, David,¬†van Huissteden, Ko,¬†Maximov, Trofim C.,¬†
                  <em>Lobry, Sylvain</em>,¬†and Heijmans, Monique M.P.D.
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>Science of The Total Environment</em> 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="https://www.sciencedirect.com/science/article/pii/S0048969721019471/pdfft?md5=67175ce2400ef6ffa16b94da8c8b47ec&amp;pid=1-s2.0-S0048969721019471-main.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://www.sciencedirect.com/science/article/pii/S0048969721019471" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Library</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Vegetation change, permafrost degradation and their interactions affect greenhouse gas fluxes, hydrology and surface energy balance in Arctic ecosystems. The Arctic shows an overall ‚Äúgreening‚Äù trend (i.e. increased plant biomass and productivity) attributed to expansion of shrub vegetation. However, Arctic shrub dynamics show strong spatial variability and locally ‚Äúbrowning‚Äù may be observed. Mechanistic understanding of greening and browning trends is necessary to accurately assess the response of Arctic vegetation to a changing climate. In this context, the Siberian Arctic is an understudied region. Between 2010 and 2019, increased browning (as derived from the MODIS Enhanced Vegetation Index) was observed in the Eastern Siberian Indigirka Lowlands. To support interpretation of local greening and browning dynamics, we quantified changes in land cover and transition probabilities in a representative tundra site in the Indigirka Lowlands using a timeseries of three very high resolution (VHR) (0.5 m) satellite images acquired between 2010 and 2019. Using spatiotemporal Potts model regularization, we substantially reduced classification errors related to optical and phenological inconsistencies in the image material. VHR images show that recent browning was associated with declines in shrub, lichen and tussock vegetation and increases in open water, sedge and especially Sphagnum vegetation. Observed formation and expansion of small open water bodies in shrub dominated vegetation suggests abrupt thaw of ice-rich permafrost. Transitions from open water to sedge and Sphagnum, indicate aquatic succession upon disturbance. The overall shift towards open water and wetland vegetation suggests a wetting trend, likely associated with permafrost degradation. Landsat data confirmed widespread expansion of surface water throughout the Indigirka Lowlands. However, the increase in the area of small water bodies observed in VHR data was not visible in Landsat-derived surface water data, which suggests that VHR data is essential for early detection of small-scale disturbances and associated vegetation change in permafrost ecosystems.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">MAGNUSSON2021146877</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Shrub decline and expansion of wetland vegetation revealed by very high resolution land cover change detection in the Siberian lowland tundra}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Science of The Total Environment}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{782}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{146877}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0048-9697}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.scitotenv.2021.146877}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S0048969721019471}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S0048969721019471/pdfft?md5=67175ce2400ef6ffa16b94da8c8b47ec&amp;pid=1-s2.0-S0048969721019471-main.pdf}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Magn√∫sson, R√∫na √ç. and Limpens, Juul and Kleijn, David and {van Huissteden}, Ko and Maximov, Trofim C. and Lobry, Sylvain and Heijmans, Monique M.P.D.}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Siberian lowland tundra, Arctic greening, Permafrost, Land cover change, Potts model, Vegetation succession}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="HUGHES2020166" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">A deep learning framework for matching of SAR and optical imagery</div>
          <!-- Author -->
          <div class="author">Hughes, Lloyd Haydn,¬†Marcos, Diego,¬†
                  <em>Lobry, Sylvain</em>,¬†Tuia, Devis,¬†and Schmitt, Michael
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>ISPRS Journal of Photogrammetry and Remote Sensing</em> 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="https://www.sciencedirect.com/science/article/pii/S0924271620302598/pdfft?md5=7dd273fad86fe7b554fa7c2ea1def329&amp;pid=1-s2.0-S0924271620302598-main.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://www.sciencedirect.com/science/article/pii/S0924271620302598" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Library</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>SAR and optical imagery provide highly complementary information about observed scenes. A combined use of these two modalities is thus desirable in many data fusion scenarios. However, any data fusion task requires measurements to be accurately aligned. While for both data sources images are usually provided in a georeferenced manner, the geo-localization of optical images is often inaccurate due to propagation of angular measurement errors. Many methods for the matching of homologous image regions exist for both SAR and optical imagery, however, these methods are unsuitable for SAR-optical image matching due to significant geometric and radiometric differences between the two modalities. In this paper, we present a three-step framework for sparse image matching of SAR and optical imagery, whereby each step is encoded by a deep neural network. We first predict regions in each image which are deemed most suitable for matching. A correspondence heatmap is then generated through a multi-scale, feature-space cross-correlation operator. Finally, outliers are removed by classifying the correspondence surface as a positive or negative match. Our experiments show that the proposed approach provides a substantial improvement over previous methods for SAR-optical image matching and can be used to register even large-scale scenes. This opens up the possibility of using both types of data jointly, for example for the improvement of the geo-localization of optical satellite imagery or multi-sensor stereogrammetry.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">HUGHES2020166</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A deep learning framework for matching of SAR and optical imagery}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ISPRS Journal of Photogrammetry and Remote Sensing}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{169}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{166-179}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0924-2716}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.isprsjprs.2020.09.012}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S0924271620302598}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S0924271620302598/pdfft?md5=7dd273fad86fe7b554fa7c2ea1def329&amp;pid=1-s2.0-S0924271620302598-main.pdf}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hughes, Lloyd Haydn and Marcos, Diego and Lobry, Sylvain and Tuia, Devis and Schmitt, Michael}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Multi-modal image matching, Image registration, Feature detection, Deep learning, Synthetic Aperture Radar (SAR), Optical imagery}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="9088993" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">RSVQA: Visual Question Answering for Remote Sensing Data</div>
          <!-- Author -->
          <div class="author">
                  <em>Lobry, Sylvain</em>,¬†Marcos, Diego,¬†Murray, Jesse,¬†and Tuia, Devis
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE Transactions on Geoscience and Remote Sensing</em> 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="http://arxiv.org/abs/2003.07333" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/9088993" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Library</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
            <a href="https://rsvqa.sylvainlobry.com" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This article introduces the task of visual question answering for remote sensing data (RSVQA). Remote sensing images contain a wealth of information, which can be useful for a wide range of tasks, including land cover classification, object counting, or detection. However, most of the available methodologies are task-specific, thus inhibiting generic and easy access to the information contained in remote sensing data. As a consequence, accurate remote sensing product generation still requires expert knowledge. With RSVQA, we propose a system to extract information from remote sensing data that is accessible to every user: we use questions formulated in natural language and use them to interact with the images. With the system, images can be queried to obtain high-level information specific to the image content or relational dependencies between objects visible in the images. Using an automatic method introduced in this article, we built two data sets (using low- and high-resolution data) of image/question/answer triplets. The information required to build the questions and answers is queried from OpenStreetMap (OSM). The data sets can be used to train (when using supervised methods) and evaluate models to solve the RSVQA task. We report the results obtained by applying a model based on convolutional neural networks (CNNs) for the visual part and a recurrent neural network (RNN) for the natural language part of this task. The model is trained on the two data sets, yielding promising results in both cases.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">9088993</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lobry, Sylvain and Marcos, Diego and Murray, Jesse and Tuia, Devis}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Geoscience and Remote Sensing}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{RSVQA: Visual Question Answering for Remote Sensing Data}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{58}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{8555-8566}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/abstract/document/9088993}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{2003.07333}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TGRS.2020.2988782}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://rsvqa.sylvainlobry.com}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="8897698" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Water Detection in SWOT HR Images Based on Multiple Markov Random Fields</div>
          <!-- Author -->
          <div class="author">
                  <em>Lobry, Sylvain</em>,¬†Denis, Lo√Øc,¬†Williams, Brent,¬†Fj√∏rtoft, Roger,¬†and Tupin, Florence
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</em> 2019
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="/assets/pdf/JSTARS2016.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://ieeexplore.ieee.org/document/8897698" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Library</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>One of the main objectives of the surface water and ocean topography (SWOT) mission, scheduled for launch in 2021, is to measure inland water levels using synthetic aperture radar (SAR) interferometry. A key step toward this objective is to precisely detect water areas. In this article, we present a method to detect water in SWOT images. Water is detected based on the relative brightness of the water and nonwater surfaces. Water brightness varies throughout the swath because of system parameters (i.e., the antenna pattern), as well as the phenomenology such as wind speed and surface roughness. To handle the effects of brightness variability, we propose to model the problem with one Markov random field (MRF) on the binary classification map, and two other MRFs to regularize the estimation of the class parameters (i.e., the land and water background power images). Our experiments show that the proposed method is more robust to the expected variations in SWOT images than traditional approaches.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">8897698</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lobry, Sylvain and Denis, Lo√Øc and Williams, Brent and Fj√∏rtoft, Roger and Tupin, Florence}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Water Detection in SWOT HR Images Based on Multiple Markov Random Fields}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4315-4326}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/8897698}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/JSTARS.2019.2948788}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{JSTARS2016.pdf}</span><span class="p">,</span>
  <span class="na">project</span> <span class="p">=</span> <span class="s">{SWOT}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="8807383" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Half a Percent of Labels is Enough: Efficient Animal Detection in UAV Imagery Using Deep CNNs and Active Learning</div>
          <!-- Author -->
          <div class="author">Kellenberger, Benjamin,¬†Marcos, Diego,¬†
                  <em>Lobry, Sylvain</em>,¬†and Tuia, Devis
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE Transactions on Geoscience and Remote Sensing</em> 2019
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="http://arxiv.org/abs/1907.07319" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a href="https://ieeexplore.ieee.org/document/8807383" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Library</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We present an Active Learning (AL) strategy for reusing a deep Convolutional Neural Network (CNN)-based object detector on a new data set. This is of particular interest for wildlife conservation: given a set of images acquired with an Unmanned Aerial Vehicle (UAV) and manually labeled ground truth, our goal is to train an animal detector that can be reused for repeated acquisitions, e.g., in follow-up years. Domain shifts between data sets typically prevent such a direct model application. We thus propose to bridge this gap using AL and introduce a new criterion called Transfer Sampling (TS). TS uses Optimal Transport (OT) to find corresponding regions between the source and the target data sets in the space of CNN activations. The CNN scores in the source data set are used to rank the samples according to their likelihood of being animals, and this ranking is transferred to the target data set. Unlike conventional AL criteria that exploit model uncertainty, TS focuses on very confident samples, thus allowing quick retrieval of true positives in the target data set, where positives are typically extremely rare and difficult to find by visual inspection. We extend TS with a new window cropping strategy that further accelerates sample retrieval. Our experiments show that with both strategies combined, less than half a percent of oracle-provided labels are enough to find almost 80% of the animals in challenging sets of UAV images, beating all baselines by a margin.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">8807383</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Kellenberger, Benjamin and Marcos, Diego and Lobry, Sylvain and Tuia, Devis}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Geoscience and Remote Sensing}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Half a Percent of Labels is Enough: Efficient Animal Detection in UAV Imagery Using Deep CNNs and Active Learning}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{57}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{9524-9533}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{1907.07319}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/8807383}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/TGRS.2019.2927393}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="VARGASMUNOZ2019283" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Correcting rural building annotations in OpenStreetMap using convolutional neural networks</div>
          <!-- Author -->
          <div class="author">Vargas-Mu√±oz, John E.,¬†
                  <em>Lobry, Sylvain</em>,¬†Falc√£o, Alexandre X.,¬†and Tuia, Devis
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>ISPRS Journal of Photogrammetry and Remote Sensing</em> 2018
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="https://www.sciencedirect.com/science/article/pii/S092427161830306X" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Library</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Rural building mapping is paramount to support demographic studies and plan actions in response to crisis that affect those areas. Rural building annotations exist in OpenStreetMap (OSM), but their quality and quantity are not sufficient for training models that can create accurate rural building maps. The problems with these annotations essentially fall into three categories: (i) most commonly, many annotations are geometrically misaligned with the updated imagery; (ii) some annotations do not correspond to buildings in the images (they are misannotations or the buildings have been destroyed); and (iii) some annotations are missing for buildings in the images (the buildings were never annotated or were built between subsequent image acquisitions). First, we propose a method based on Markov Random Field (MRF) to align the buildings with their annotations. The method maximizes the correlation between annotations and a building probability map while enforcing that nearby buildings have similar alignment vectors. Second, the annotations with no evidence in the building probability map are removed. Third, we present a method to detect non-annotated buildings with predefined shapes and add their annotation. The proposed methodology shows considerable improvement in accuracy of the OSM annotations for two regions of Tanzania and Zimbabwe, being more accurate than state-of-the-art baselines.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">VARGASMUNOZ2019283</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Correcting rural building annotations in OpenStreetMap using convolutional neural networks}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ISPRS Journal of Photogrammetry and Remote Sensing}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{147}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{283-293}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0924-2716}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.isprsjprs.2018.11.010}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S092427161830306X}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Vargas-Mu√±oz, John E. and Lobry, Sylvain and Falc√£o, Alexandre X. and Tuia, Devis}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Very high resolution mapping, Convolutional neural networks, Shape priors, OpenStreetMap, Volunteered geographical information, Update of vector maps}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="Shivangi_LU" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Fine-grained landuse characterization using ground-based pictures: a deep learning solution based on globally available data</div>
          <!-- Author -->
          <div class="author">Srivastava, Shivangi,¬†Mu√±oz, John E. Vargas,¬†
                  <em>Lobry, Sylvain</em>,¬†and Tuia, Devis
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>International Journal of Geographical Information Science</em> 2018
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="https://www.tandfonline.com/doi/pdf/10.1080/13658816.2018.1542698?needAccess=true" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a href="https://doi.org/10.1080/13658816.2018.1542698" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Library</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p> ABSTRACTWe study the problem of landuse characterization at the urban-object level using deep learning algorithms. Traditionally, this task is performed by surveys or manual photo interpretation, which are expensive and difficult to update regularly. We seek to characterize usages at the single object level and to differentiate classes such as educational institutes, hospitals and religious places by visual cues contained in side-view pictures from Google Street View (GSV). These pictures provide geo-referenced information not only about the material composition of the objects but also about their actual usage, which otherwise is difficult to capture using other classical sources of data such as aerial imagery. Since the GSV database is regularly updated, this allows to consequently update the landuse maps, at lower costs than those of authoritative surveys. Because every urban-object is imaged from a number of viewpoints with street-level pictures, we propose a deep-learning based architecture that accepts arbitrary number of GSV pictures to predict the fine-grained landuse classes at the object level. These classes are taken from OpenStreetMap. A quantitative evaluation of the area of √éle-de-France, France shows that our model outperforms other deep learning-based methods, making it a suitable alternative to manual landuse characterization.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Shivangi_LU</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Srivastava, Shivangi and Mu√±oz, John E. Vargas and Lobry, Sylvain and Tuia, Devis}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Fine-grained landuse characterization using ground-based pictures: a deep learning solution based on globally available data}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Journal of Geographical Information Science}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{34}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1117-1136}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Taylor &amp; Francis}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1080/13658816.2018.1542698}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://www.tandfonline.com/doi/pdf/10.1080/13658816.2018.1542698?needAccess=true}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1080/13658816.2018.1542698}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1080/13658816.2018.1542698}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2016</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="7488202" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Multitemporal SAR Image Decomposition into Strong Scatterers, Background, and Speckle</div>
          <!-- Author -->
          <div class="author">
                  <em>Lobry, Sylvain</em>,¬†Denis, Lo√Øc,¬†and Tupin, Florence
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</em> 2016
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="/assets/pdf/JSTARS2016.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://ieeexplore.ieee.org/document/7488202" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Library</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Speckle phenomenon in synthetic aperture radar (SAR) images makes their visual and automatic interpretation a difficult task. To reduce strong fluctuations due to speckle, total variation (TV) regularization has been proposed by several authors to smooth out noise without blurring edges. A specificity of SAR images is the presence of strong scatterers having a radiometry several orders of magnitude larger than their surrounding region. These scatterers, especially present in urban areas, limit the effectiveness of TV regularization as they break the assumption of an image made of regions of constant radiometry. To overcome this limitation, we propose in this paper an image decomposition approach. There exist numerous methods to decompose an image into several components, notably to separate textural and geometrical information. These decomposition models are generally recast as energy minimization problems involving a different penalty term for each of the components. In this framework, we propose an energy suitable for the decomposition of SAR images into speckle, a smooth background, and strong scatterers, and discuss its minimization using max-flow/min-cut algorithms. We make the connection between the minimization problem considered, involving the L0 pseudonorm, and the generalized likelihood ratio test used in detection theory. The proposed decomposition jointly performs the detection of strong scatterers and the estimation of the background radiometry. Given the increasing availability of time series of SAR images, we consider the decomposition of a whole time series. New change detection methods can be based on the temporal analysis of the components obtained from our decomposition.</p>
          </div>
        </div>
      </div>
</li></ol>


</div>
<p><br><br></p>
<div class="publications">
<h2>International Conferences</h2>
  <h2 class="year">2023</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="Leygonie2023MVEO" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">An a contrario approach for plant disease detection (to appear)</div>
          <!-- Author -->
          <div class="author">Leygonie, Rebecca,¬†
                  <em>Lobry, Sylvain</em>,¬†and Wendling, Laurent
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Workshop on Machine Vision for Earth Observation at BMVC</em> 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Detecting plant diseases or abnormalities is not a trivial task, as they can be caused by multiple factors such as environmental conditions, genetics, pathogens, etc.  Because there is a need to help farmers make decisions to maximize crop yields, many studies have emerged in recent years using deep learning on agricultural images to detect plant diseases, which can be considered as an anomaly detection task. However, these approaches are often limited by the availability of annotated data or prior knowledge of the existence of an anomaly. We propose an approach that can detect part of the anomalies without prior knowledge of their existence, thus overcoming some of these limitations. To this end, we train a model on an auxiliary prediction task (plants‚Äô age regression). We then use an explicability model to retrieve heatmaps whose distributions are studied. For each new observation, we propose to study how closely its heatmap follows the desired distribution and we derive a score indicating potential anomalies. Experiments on the GrowliFlower dataset indicate how our proposed method can help potential end-user to automatically find anomalies.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Leygonie2023MVEO</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{An a contrario approach for plant disease detection (to appear)}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Leygonie, Rebecca and Lobry, Sylvain and Wendling, Laurent}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Workshop on Machine Vision for Earth Observation at BMVC}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="Chappuis2023MVEO" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Multi-task prompt-RSVQA to explicitly count objects on aerial images (to appear)</div>
          <!-- Author -->
          <div class="author">Chappuis, Christel,¬†Sertic, Charlotte,¬†Santacroce, Nicolas,¬†Castillo Navarro, Javiera,¬†
                  <em>Lobry, Sylvain</em>,¬†Le Saux, Bertrand,¬†and Tuia, Devis
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Workshop on Machine Vision for Earth Observation at BMVC</em> 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Introduced to enable a wider use of Earth Observation images using natural language, Remote Sensing Visual Question Answering (RSVQA) remains a challenging task, in particular for questions related to counting. To address this specific challenge, we propose a modular Multi-task prompt-RSVQA model based on object detection and question answering modules. By creating a semantic bottleneck describing the image and providing a visual answer, our model allows users to assess the visual grounding of the answer and better interpret the prediction. A set of ablation studies are designed to consider the contributions of different modules and evaluation metrics are discussed for a finer-grained assessment. Experiments demonstrate competitive results against literature baselines and a zero-shot VQA model. In particular, our proposed model predicts answers for numerical Counting questions that are consistently closer in distance to the ground truth.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Chappuis2023MVEO</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multi-task prompt-RSVQA to explicitly count objects on aerial images (to appear)}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chappuis, Christel and Sertic, Charlotte and Santacroce, Nicolas and Castillo Navarro, Javiera and Lobry, Sylvain and Le Saux, Bertrand and Tuia, Devis}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Workshop on Machine Vision for Earth Observation at BMVC}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="Leygonie2023ICIP" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Transforming multidimensional data into images to overcome the curse of dimensionality (to appear)</div>
          <!-- Author -->
          <div class="author">Leygonie, Rebecca,¬†
                  <em>Lobry, Sylvain</em>,¬†Vimont, Guillaume,¬†and Wendling, Laurent
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In IEEE International Conference on Image Processing ICIP</em> 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>When dealing with high-dimensional multivariate time series classification problems, a well-known difficulty is the \textitcurse of dimensionality. 
In this article, we propose an original approach of transposition of multidimensional data into images to tackle the task of classification. We propose a lightweight hybrid model that take this transposed data as an input. This model contains  convolutional layers as a feature extractor followed by a recurrent neural network. We apply our method to a large dataset consisting of individual patient medical records. We show that our approach allows us to significantly reduce the size of a network and increase its performance by opting for a transformation of the input data.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Leygonie2023ICIP</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Transforming multidimensional data into images to overcome the curse of dimensionality (to appear)}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Leygonie, Rebecca and Lobry, Sylvain and Vimont, Guillaume and Wendling, Laurent}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Conference on Image Processing ICIP}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="Letheule2023IGARSS" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Automatic simulation of SAR images: comparing a deep-learning based method to a hybrid method (to appear)</div>
          <!-- Author -->
          <div class="author">Letheule, Nathan,¬†Weissgerber, Flora,¬†
                  <em>Lobry, Sylvain</em>,¬†and Colin, Elise
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In IEEE International Geoscience and Remote Sensing Symposium IGARSS</em> 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Letheule2023IGARSS</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automatic simulation of SAR images: comparing a deep-learning based method to a hybrid method (to appear)}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Letheule, Nathan and Weissgerber, Flora and Lobry, Sylvain and Colin, Elise}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Geoscience and Remote Sensing Symposium IGARSS}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="Rousse2023Linking" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Linking population data to high resolution maps: a case study in Burkina Faso</div>
          <!-- Author -->
          <div class="author">Rousse, Basile,¬†
                  <em>Lobry, Sylvain</em>,¬†Duth√©, G√©raldine,¬†Golaz, Val√©rie,¬†and Wendling, Laurent
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Machine Learning for Remote Sensing at ICLR (oral presentation)</em> 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="/assets/pdf/ICLR23.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Recent research in demography focuses on linking population data to environmental indicators. Satellite imagery can support such projects by providing data at a large scale and a high frequency. Moreover, population surveys often provide geolocations of households, yet sometimes with an offset, to guarantee data confidentiality. In such cases, the proper management of this incertitude is required, to accurately link environmental indicators such as land cover/land use maps or spectral indices to population data. In this paper, we introduce a method based on the random sampling of possible households geolocations around the coordinates provided. Then, we link a land cover map generated using semi-supervised deep learning and a Malaria Indicator Survey in Burkina Faso. After linking households to their close environment, we distinguish several types of environment conducive to high malaria rates, beyond the urban/rural dichotomy.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Rousse2023Linking</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Linking population data to high resolution maps: a case study in Burkina Faso}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rousse, Basile and Lobry, Sylvain and Duth√©, G√©raldine and Golaz, Val√©rie and Wendling, Laurent}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Machine Learning for Remote Sensing at ICLR (oral presentation)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{ICLR23.pdf}</span><span class="p">,</span>
  <span class="na">project</span> <span class="p">=</span> <span class="s">{DEMO}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="Rousse2023Seasonal" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Seasonal semi-supervised domain adaptation for linking population studies and Local Climate Zones</div>
          <!-- Author -->
          <div class="author">Rousse, Basile,¬†
                  <em>Lobry, Sylvain</em>,¬†Duth√©, G√©raldine,¬†Golaz, Val√©rie,¬†and Wendling, Laurent
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Joint Urban Remote Sensing Event (JURSE)</em> 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Environment and demographic dynamics are strongly linked. However, relevant data to study this interaction may be scarce especially in sub-Saharan Africa where it is not always possible to perform such studies with a high temporal frequency. Satellite imagery, when linked to demographic data, can be a significant asset to estimate missing data as it covers every country with both high spatial and temporal resolution. We aim to take advantage of satellite data to characterize the environment in inter-tropical areas. This environment is regulated by the changing of two seasons that are essential to consider. We introduce a semi-supervised domain adaptation strategy for neural networks based on seasonal changes. This strategy can be used to produce land cover maps in regions of the world where limited labeled datasets are available. We apply this method to produce environmental indicators and link them to malaria rates from the Malaria Indicator Survey of Burkina Faso. We show that malaria rates are correlated not only to urbanisation but also to the environmental characterisation of studied areas.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Rousse2023Seasonal</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Seasonal semi-supervised domain adaptation for linking population studies and Local Climate Zones}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rousse, Basile and Lobry, Sylvain and Duth√©, G√©raldine and Golaz, Val√©rie and Wendling, Laurent}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Joint Urban Remote Sensing Event (JURSE)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">project</span> <span class="p">=</span> <span class="s">{DEMO}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="Chappuis2022Prompting" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Prompt‚ÄìRSVQA: Prompting visual context to a language model for Remote Sensing Visual Question Answering</div>
          <!-- Author -->
          <div class="author">Chappuis, Chritel,¬†Zermatten, Valerie,¬†
                  <em>Lobry, Sylvain</em>,¬†Le Saux, Bertrand,¬†and Tuia, Devis
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In EarthVision at IEEE/CVF Conference on Computer Vision and Pattern Recognition CVPR</em> 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Remote sensing visual question answering (RSVQA) was recently proposed with the aim of interfacing natural language and vision to ease the access of information contained in Earth Observation data for a wide audience, which is granted by simple questions in natural language. The traditional vision/language interface is an embedding obtained by fusing features from two deep models, one processing the image and another the question. Despite the success of early VQA models, it remains difficult to control the adequacy of the visual information extracted by its deep model, which should act as a context regularizing the work of the language model. We propose to extract this context information with a visual model, convert it to text and inject it, i.e. prompt it, into a language model. The language model is therefore responsible to process the question with the visual context, and extract features, which are useful to find the answer. We study the effect of prompting with respect to a black-box visual extractor and discuss the importance of training a visual model producing accurate context.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Chappuis2022Prompting</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Prompt‚ÄìRSVQA: Prompting visual context to a language model for Remote Sensing Visual Question Answering}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chappuis, Chritel and Zermatten, Valerie and Lobry, Sylvain and Le Saux, Bertrand and Tuia, Devis}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{EarthVision at IEEE/CVF Conference on Computer Vision and Pattern Recognition CVPR}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="Faure2022Embedding" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Embedding Spatial Relations in Visual Question Answering for Remote Sensing</div>
          <!-- Author -->
          <div class="author">Faure, Maxime,¬†
                  <em>Lobry, Sylvain</em>,¬†Kurtz, Camille,¬†and Wendling, Laurent
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 26TH International Conference on Pattern Recognition ICPR</em> 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Remote sensing images carry a wealth of information that is not easily accessible to end-users as it requires strong technical skills and knowledge. 
Visual Question Answering (VQA), a task that aims at answering an open-ended question in natural language from an image, can provide an easier access to this information. Considering the geographical information contained in remote sensing images, questions often embed an important spatial aspect, for instance regarding the relative position of two objects. Our objective is to better model the spatial relations in the construction of a ground-truth database of image/question/answer triplets and to assess the capacity a VQA model has to answer these questions. In this article, we propose to use histograms of forces to model the directional spatial relations between geo-localized objects. This allows a finer modeling of ambiguous relationships between objects and to provide different levels of assessment of a relation (e.g. object A is slightly/strictly to the west of object B).  Using this new dataset, we evaluate the performances of a classical VQA model and propose a curriculum learning strategy to better take into account the varying difficulty of questions embedding spatial relations.  With this approach, we show an improvement in the performances of our model, highlighting the interest of embedding spatial relations in VQA for remote sensing applications.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Faure2022Embedding</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Embedding Spatial Relations in Visual Question Answering for Remote Sensing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Faure, Maxime and Lobry, Sylvain and Kurtz, Camille and Wendling, Laurent}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{26TH International Conference on Pattern Recognition ICPR}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="chappuis2022Language" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Language transformers for remote sensing visual question answering</div>
          <!-- Author -->
          <div class="author">Chappuis, Christel,¬†Mendez, Vincent,¬†Walt, Eliot,¬†
                  <em>Lobry, Sylvain</em>,¬†Le Saux, Bertrand,¬†and Tuia, Devis
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In IEEE International Geoscience and Remote Sensing Symposium IGARSS</em> 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Remote sensing visual question answering (RSVQA) opens new avenues to promote the use of satellites data, by interfacing satellite image analysis with natural language processing. Capitalizing on the remarkable advances in natural language processing and computer vision, RSVQA aims at finding an answer to a question formulated by a human user about a remote sensing image. This is achieved by extracting representations from images and questions, and then fusing them in a joint representation. Focusing on the language part of the architecture, this study compares and evaluates the adequacy to the RSVQA task of two language models, a traditional recurrent neural network (Skip-thoughts) and a recent attention-based Transformer (BERT). We study whether large transformer models are beneficial to the task and whether fine-tuning is needed for these models to perform at their best. Our findings show that the models benefit from fine-tuning language models and that RSVQA with BERT is slightly but consistently better when properly fine-tuned.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chappuis2022Language</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Language transformers for remote sensing visual question answering}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chappuis, Christel and Mendez, Vincent and Walt, Eliot and Lobry, Sylvain and Le Saux, Bertrand and Tuia, Devis}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE International Geoscience and Remote Sensing Symposium IGARSS}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="Thay2022LPS" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Matching environmental data produced from remote sensing images to demographic data in Sub-Saharan Africa</div>
          <!-- Author -->
          <div class="author">Thay*, Lys,¬†Rousse*, Basile,¬†
                  <em>Lobry, Sylvain</em>,¬†Duth√©, G√©raldine,¬†Wendling, Laurent,¬†and Golaz, Val√©rie
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In ESA Living Planet Symposium</em> 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Thay2022LPS</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Matching environmental data produced from remote sensing images to demographic data in Sub-Saharan Africa}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Thay*, Lys and Rousse*, Basile and Lobry, Sylvain and Duth√©, G√©raldine and Wendling, Laurent and Golaz, Val√©rie}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ESA Living Planet Symposium}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">project</span> <span class="p">=</span> <span class="s">{DEMO}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="chappuis2021find" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">How to find a good image-text embedding for remote sensing visual question answering?</div>
          <!-- Author -->
          <div class="author">Chappuis, Christel,¬†
                  <em>Lobry, Sylvain</em>,¬†Kellenberger, Benjamin,¬†Le Saux, Bertrand,¬†and Tuia, Devis
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In MACLEAN Workshop at ECML/PKDD 2021</em> 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="http://arxiv.org/abs/2109.11848" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Visual question answering (VQA) has recently been intro- duced to remote sensing to make information extraction from overhead imagery more accessible to everyone. VQA considers a question (in nat- ural language, therefore easy to formulate) about an image and aims at providing an answer through a model based on computer vision and natu- ral language processing methods. As such, a VQA model needs to jointly consider visual and textual features, which is frequently done through a fusion step. In this work, we study three different fusion methodologies in the context of VQA for remote sensing and analyse the gains in ac- curacy with respect to the model complexity. Our findings indicate that more complex fusion mechanisms yield an improved performance, yet that seeking a trade-off between model complexity and performance is worthwhile in practice.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chappuis2021find</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{How to find a good image-text embedding for remote sensing visual question answering?}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chappuis, Christel and Lobry, Sylvain and Kellenberger, Benjamin and Le Saux, Bertrand and Tuia, Devis}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{MACLEAN Workshop at ECML/PKDD 2021}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{2109.11848}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="lobry2021rsvqa" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">RSVQA Meets Bigearthnet: A New, Large-Scale, Visual Question Answering Dataset for Remote Sensing</div>
          <!-- Author -->
          <div class="author">
                  <em>Lobry, Sylvain</em>,¬†Demir, Beg√ºm,¬†and Tuia, Devis
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS</em> 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="/assets/pdf/IGARSS2021.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
            <a href="https://rsvqa.sylvainlobry.com" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Visual Question Answering is a new task that can facilitate the extraction of information from images through textual queries: it aims at answering an open-ended question for- mulated in natural language about a given image. In this work, we introduce a new dataset to tackle the task of visual question answering on remote sensing images: this large- scale, open access dataset extracts image/question/answer triplets from the BigEarthNet dataset. This new dataset contains close to 15 millions samples and is openly avail- able. We present the dataset construction procedure, its characteristics and first results using a deep-learning based methodology. These first results show that the task of vi- sual question answering is challenging and opens new in- teresting research avenues at the interface of remote sensing and natural language processing. The dataset and the code to create and process it are open and freely available on https://rsvqa.sylvainlobry.com/</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lobry2021rsvqa</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{RSVQA Meets Bigearthnet: A New, Large-Scale, Visual Question Answering Dataset for Remote Sensing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lobry, Sylvain and Demir, Beg{\"u}m and Tuia, Devis}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1218--1221}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{IGARSS2021.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">code</span> <span class="p">=</span> <span class="s">{https://rsvqa.sylvainlobry.com}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="lobry2020better" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Better Generic Objects Counting When Asking Questions to Images: A Multitask Approach for Remote Sensing Visual Question Answering</div>
          <!-- Author -->
          <div class="author">
                  <em>Lobry, Sylvain</em>,¬†Marcos, Diego,¬†Kellenberger, Benjamin,¬†and Tuia, Devis
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences</em> 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a href="/assets/pdf/ISPRS2020.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lobry2020better</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Better Generic Objects Counting When Asking Questions to Images: A Multitask Approach for Remote Sensing Visual Question Answering}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lobry, Sylvain and Marcos, Diego and Kellenberger, Benjamin and Tuia, Devis}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1021--1027}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">absract</span> <span class="p">=</span> <span class="s">{Visual Question Answering for Remote Sensing (RSVQA) aims at extracting information from remote sensing images through queries formulated in natural language. Since the answer to the query is also provided in natural language, the system is accessible to non-experts, and therefore dramatically increases the value of remote sensing images as a source of information, for example for journalism purposes or interactive land planning. Ideally, an RSVQA system should be able to provide an answer to questions that vary both in terms of topic (presence, localization, counting) and image content. However, aiming at such flexibility generates problems related to the variability of the possible answers. A striking example is counting, where the number of objects present in a remote sensing image can vary by multiple orders of magnitude, depending on both the scene and type of objects. This represents a challenge for traditional Visual Question Answering (VQA) methods, which either become intractable or result in an accuracy loss, as the number of possible answers has to be limited. To this end, we introduce a new model that jointly solves a classification problem (which is the most common approach in VQA) and a regression problem (to answer numerical questions more precisely). An evaluation of this method on the RSVQA dataset shows that this finer numerical output comes at the cost of a small loss of performance on non-numerical questions.}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{ISPRS2020.pdf}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="hua2020learning" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Learning multi-label aerial image classification under label noise: A regularization approach using word embeddings</div>
          <!-- Author -->
          <div class="author">Hua, Yuansheng,¬†
                  <em>Lobry, Sylvain</em>,¬†Mou, Lichao,¬†Tuia, Devis,¬†and Zhu, Xiao Xiang
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In IGARSS 2020-2020 IEEE International Geoscience and Remote Sensing Symposium</em> 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="/assets/pdf/IGARSS2020.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Training deep neural networks requires well-annotated datasets. However, real world datasets are often noisy, es- pecially in a multi-label scenario, i.e. where each data point can be attributed to more than one class. To this end, we propose a regularization method to learn multi-label classifi- cation networks from noisy data. This regularization is based on the assumption that semantically close classes are more likely to appear together in a given image. Hereby, we encode label correlations with prior knowledge and regularize noisy network predictions using label correlations. To evaluate its effectiveness, we perform experiments on a mutli-label aerial image dataset contaminated with controlled levels of label noise. Results indicate that networks trained using the pro- posed method outperform those directly learned from noisy labels and that the benefits increase proportionally to the amount of noise present.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hua2020learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning multi-label aerial image classification under label noise: A regularization approach using word embeddings}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hua, Yuansheng and Lobry, Sylvain and Mou, Lichao and Tuia, Devis and Zhu, Xiao Xiang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IGARSS 2020-2020 IEEE International Geoscience and Remote Sensing Symposium}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{525--528}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{IGARSS2020.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="levering2020interpretable" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Interpretable Scenicness from Sentinel-2 Imagery</div>
          <!-- Author -->
          <div class="author">Levering, Alex,¬†Marcos, Diego,¬†
                  <em>Lobry, Sylvain</em>,¬†and Tuia, Devis
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In IGARSS 2020-2020 IEEE International Geoscience and Remote Sensing Symposium</em> 2020
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Landscape aesthetics, or scenicness, has been identified as an important ecosystem service that contribute to human health and well-being. Currently there are no methods to inventorize landscape scenicness on a large scale. In this paper we study how to upscale local assessments of scenicness provided by human observers, and we do so by using satellite images. Moreover, we develop an explicitly interpretable CNN model that allows assessing the connections between landscape scenicness and the presence of specific landcover types. To generate the landscape scenicness ground truth, we use the ScenicOrNot crowdsourcing database, which provides geo-referenced, human-based scenicness estimates for ground based photos in Great Britain. Our results show that it is feasible to predict landscape scenicness based on satellite imagery. The interpretable model performs comparably to an unconstrained model, suggesting that it is possible to learn a semantic bottleneck that represents well the present landcover classes and still contains enough information to accurately predict the location‚Äôs scenicness.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">levering2020interpretable</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Interpretable Scenicness from Sentinel-2 Imagery}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Levering, Alex and Marcos, Diego and Lobry, Sylvain and Tuia, Devis}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IGARSS 2020-2020 IEEE International Geoscience and Remote Sensing Symposium}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3983--3986}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="marcos2019semantically" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Semantically Interpretable Activation Maps: what-where-how explanations within CNNs</div>
          <!-- Author -->
          <div class="author">Marcos, Diego,¬†
                  <em>Lobry, Sylvain</em>,¬†and Tuia, Devis
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</em> 2019
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="http://arxiv.org/abs/1909.08442" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>A main issue preventing the use of Convolutional Neural Networks (CNN) in end user applications is the low level of transparency in the decision process. Previous work on CNN interpretability has mostly focused either on localizing the regions of the image that contribute to the result or on building an external model that generates plausible explanations. However, the former does not provide any semantic information and the latter does not guarantee the faithfulness of the explanation. We propose an intermediate representation composed of multiple Semantically Interpretable Activation Maps (SIAM) indicating the presence of predefined attributes at different locations of the image. These attribute maps are then linearly combined to produce the final output. This gives the user insight into what the model has seen, where, and a final output directly linked to this information in a comprehensive and interpretable way. We test the method on the task of landscape scenicness (aesthetic value) estimation, using an intermediate representation of 33 attributes from the SUN Attributes database. The results confirm that SIAM makes it possible to understand what attributes in the image are contributing to the final score and where they are located. Since it is based on learning from multiple tasks and datasets, SIAM improve the explanability of the prediction without additional annotation efforts or computational overhead at inference time, while keeping good performances on both the final and intermediate tasks.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">marcos2019semantically</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Semantically Interpretable Activation Maps: what-where-how explanations within CNNs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Marcos, Diego and Lobry, Sylvain and Tuia, Devis}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4207--4215}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{1909.08442}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="lobry2019visual" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Visual question answering from remote sensing images</div>
          <!-- Author -->
          <div class="author">
                  <em>Lobry, Sylvain</em>,¬†Murray, Jesse,¬†Marcos, Diego,¬†and Tuia, Devis
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium</em> 2019
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/8898891" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Library</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Remote sensing images carry wide amounts of information beyond land cover or land use. Images contain visual and structural information that can be queried to obtain high level information about specific image content or relational dependencies between the objects sensed. This paper explores the possibility to use questions formulated in natural language as a generic and accessible way to extract this type of information from remote sensing images, i.e. visual question answering. We introduce an automatic way to create a dataset using OpenStreetMap 1 data and present some preliminary results. Our proposed approach is based on deep learning, and is trained using our new dataset.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lobry2019visual</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Visual question answering from remote sensing images}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lobry, Sylvain and Murray, Jesse and Marcos, Diego and Tuia, Devis}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4951--4954}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/abstract/document/8898891}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="lobry2019deep" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Deep learning models to count buildings in high-resolution overhead images</div>
          <!-- Author -->
          <div class="author">
                  <em>Lobry, Sylvain</em>,¬†and Tuia, Devis
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 2019 Joint Urban Remote Sensing Event (JURSE)</em> 2019
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="/assets/pdf/JURSE2019.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/8809058" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Library</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This paper addresses the problem of counting buildings in very high-resolution overhead true color imagery. We study and discuss the relevance of deep-learning based methods to this task. Two architectures and two loss functions are proposed and compared. We show that a model enforcing equivariance to rotations is beneficial for the task of counting in remotely sensed images. We also highlight the importance of robustness to outliers of the loss function when considering remote sensing applications.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lobry2019deep</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Deep learning models to count buildings in high-resolution overhead images}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lobry, Sylvain and Tuia, Devis}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2019 Joint Urban Remote Sensing Event (JURSE)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--4}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/abstract/document/8809058}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{JURSE2019.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="marcos2018scale" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Scale equivariance in CNNs with vector fields</div>
          <!-- Author -->
          <div class="author">Marcos, Diego,¬†Kellenberger, Benjamin,¬†
                  <em>Lobry, Sylvain</em>,¬†and Tuia, Devis
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In International Conference on Machine Learning (ICML)/FAIM workshop on Towards learning with limited labels: Equivariance, Invariance, and Beyond</em> 2018
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="http://arxiv.org/abs/1807.11783" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We study the effect of injecting local scale equivariance into Convolutional Neural Networks. This is done by applying each convolutional filter at multiple scales. The output is a vector field encoding for the maximally activating scale and the scale itself, which is further processed by the following convolutional layers. This allows all the intermediate representations to be locally scale equivariant. We show that this improves the performance of the model by over 20% in the scale equivariant task of regressing the scaling factor applied to randomly scaled MNIST digits. Furthermore, we find it also useful for scale invariant tasks, such as the actual classification of randomly scaled digits. This highlights the usefulness of allowing for a compact representation that can also learn relationships between different local scales by keeping internal scale equivariance.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">marcos2018scale</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Scale equivariance in CNNs with vector fields}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Marcos, Diego and Kellenberger, Benjamin and Lobry, Sylvain and Tuia, Devis}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning (ICML)/FAIM workshop on Towards learning with limited labels: Equivariance, Invariance, and Beyond}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">arxiv</span> <span class="p">=</span> <span class="s">{1807.11783}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="vargas2018correcting" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Correcting misaligned rural building annotations in open street map using convolutional neural networks evidence</div>
          <!-- Author -->
          <div class="author">Vargas-Munoz, John E,¬†Marcos, Diego,¬†
                  <em>Lobry, Sylvain</em>,¬†Santos, Jefersson A,¬†Falcao, Alexandre X,¬†and Tuia, Devis
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium</em> 2018
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/8518711" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Library</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Mapping rural buildings in developing countries is crucial to monitor and plan in those vulnerable areas. Despite the existence of some rural building annotations in OpenStreetMap (OSM), those are of insufficient quantity and quality to train models able to map large areas accurately. In particular, these annotations are very often misaligned with respect to the buildings that are present in updated aerial imagery. We propose a Markov Random Field (MRF) method to correct misaligned rural building annotations. To do so, our method uses i) the correlation between candidate aligned OSM annotations and buildings roughly detected on aerial images and ii) the local consistency of the alignment vectors.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">vargas2018correcting</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Correcting misaligned rural building annotations in open street map using convolutional neural networks evidence}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Vargas-Munoz, John E and Marcos, Diego and Lobry, Sylvain and dos Santos, Jefersson A and Falcao, Alexandre X and Tuia, Devis}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1284--1287}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/abstract/document/8518711}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="srivastava2018land" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Land-use characterisation using Google Street View pictures and OpenStreetMap</div>
          <!-- Author -->
          <div class="author">Srivastava, Shivangi,¬†
                  <em>Lobry, Sylvain</em>,¬†Tuia, Devis,¬†and Munoz, John Vargas
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 21st AGILE Conference on Geographic Information Science (2018)</em> 2018
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="/assets/pdf/AGILE2018.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This paper presents a study on the use of freely available, geo-referenced pictures from Google Street View to model and predict land-use at the urban-objects scale. This task is traditionally done manually and via photointerpretation, which is very time consuming. We propose to use a machine learning approach based on deep learning and to model land-use directly from both the pictures available from Google Street View and OpenStreetMap annotations. Because of the large availability of these two data sources, the proposed approach is scalable to cities around the globe and presents the possibility of frequent updates of the map. As base information, we use features extracted from single pictures around the object of interest; these features are issued from pre-trained convolutional neural networks. Then, we train various classifiers (Linear and RBF support vector machines, multi layer perceptron) and compare their performances. We report on a study over the city of Paris, France, where we observed that pictures coming from both inside and outside the urban-objects capture distinct, but complementary features.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">srivastava2018land</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Land-use characterisation using Google Street View pictures and OpenStreetMap}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Srivastava, Shivangi and Lobry, Sylvain and Tuia, Devis and Munoz, John Vargas}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{21st AGILE Conference on Geographic Information Science (2018)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{AGILE2018.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="8438063" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Speckle reduction in PolSAR by multi-channel variance stabilization and Gaussian denoising: MuLoG</div>
          <!-- Author -->
          <div class="author">Deledalle, Charles-Alban,¬†Denis, Loic,¬†Tupin, Florence,¬†and <em>Lobry, Sylvain</em>
                
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In EUSAR 2018; 12th European Conference on Synthetic Aperture Radar</em> 2018
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="https://ieeexplore.ieee.org/document/8438063" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Library</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Due to speckle phenomenon, some form of filtering must be applied to SAR data prior to performing any polarimetric analysis. Beyond the simple multilooking operation (i.e., moving average), several methods have been designed specifically for PolSAR filtering. The specifics of speckle noise and the correlations between polarimetric channels make PolSAR filtering more challenging than usual image restoration problems. Despite their striking performance, existing image denoising algorithms, mostly designed for additive white Gaussian noise, cannot be directly applied to PolSAR data. We bridge this gap with MuLoG by providing a general scheme that stabilizes the variance of the polarimetric channels and that can embed almost any Gaussian denoiser. We describe MuLoG approach and illustrate its performance on airborne PolSAR data using a very recent Gaussian denoiser based on a convolutional neural network.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">8438063</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Deledalle, Charles-Alban and Denis, Loic and Tupin, Florence and Lobry, Sylvain}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{EUSAR 2018; 12th European Conference on Synthetic Aperture Radar}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Speckle reduction in PolSAR by multi-channel variance stabilization and Gaussian denoising: MuLoG}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2018}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-5}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/8438063}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2017</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="8127445" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Double MRF for water classification in SAR images by joint detection and reflectivity estimation</div>
          <!-- Author -->
          <div class="author">
                  <em>Lobry, Sylvain</em>,¬†Denis, Lo√Øc,¬†Tupin, Florence,¬†and Fjortoft, Roger
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)</em> 2017
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="/assets/pdf/IGARSS2017_MRF.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://ieeexplore.ieee.org/document/8127445" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Library</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Classification of SAR images is a challenging task as the radiometric properties of a class may not be constant throughout the image. The assumption made in most classification algorithms that a class can be modeled by constant parameters is then not valid. In this paper, we propose a classification algorithm based on two Markov random fields that accounts for local and global variations of the parameters inside the image and produces a regularized classification. This algorithm is applied on airborne TropiSAR and simulated SWOT HR data. Both quantitative and visual results are provided, demonstrating the effectiveness of the proposed method.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">8127445</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lobry, Sylvain and Denis, Lo√Øc and Tupin, Florence and Fjortoft, Roger}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Double MRF for water classification in SAR images by joint detection and reflectivity estimation}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2283-2286}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/IGARSS.2017.8127445}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/8127445}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{IGARSS2017_MRF.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">project</span> <span class="p">=</span> <span class="s">{SWOT}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="8127807" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Unsupervised detection of thin water surfaces in SWOT images based on segment detection and connection</div>
          <!-- Author -->
          <div class="author">
                  <em>Lobry, Sylvain</em>,¬†Tupin, Florence,¬†and Fjortoft, Roger
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)</em> 2017
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="/assets/pdf/IGARSS2017_Rivers.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://ieeexplore.ieee.org/document/8127807" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Library</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The objective of the Surface Water and Ocean Topography (SWOT) mission is to regularly monitor the height of the earth‚Äôs water surfaces. One of the challenges toward obtaining global measurements of these surfaces is to detect small water areas. In this article we introduce a method for the detection of thin water surfaces, such as rivers, in SWOT images. It combines a low-level step (segment detection) with a high-level regularization of these features. The method is then tested on a simulated SWOT image.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">8127807</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lobry, Sylvain and Tupin, Florence and Fjortoft, Roger}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Unsupervised detection of thin water surfaces in SWOT images based on segment detection and connection}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3720-3723}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/IGARSS.2017.8127807}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/8127807}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{IGARSS2017_Rivers.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">project</span> <span class="p">=</span> <span class="s">{SWOT}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="8035245" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Urban area change detection based on generalized likelihood ratio test</div>
          <!-- Author -->
          <div class="author">Zhao, Weiying,¬†
                  <em>Lobry, Sylvain</em>,¬†Maitre, Henri,¬†Nicolas, Jean-Marie,¬†and Tupin, Florence
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 2017 9th International Workshop on the Analysis of Multitemporal Remote Sensing Images (MultiTemp)</em> 2017
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="/assets/pdf/Multitemp2017.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://ieeexplore.ieee.org/document/8035245" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Library</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Change detection methods often use denoised data because the original speckle noise has a strong influence on the detection results. The effect of using different data sources (different equivalent number of looks, original data, denoised data) and different threshold methods are studied based on four kinds of generalized likelihood ratio test approaches. NL-SAR [1] denoised data and the corresponding spatially varying equivalent number of looks are taken into account in the detection procedure. The bi-temporal experimental results on simulated data, realistic synthetic Sentinel-1 SAR data show the improvement of using equivalent number of looks of denoised data and corresponding adaptive thresholds for change detection in urban areas.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">8035245</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhao, Weiying and Lobry, Sylvain and Maitre, Henri and Nicolas, Jean-Marie and Tupin, Florence}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2017 9th International Workshop on the Analysis of Multitemporal Remote Sensing Images (MultiTemp)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Urban area change detection based on generalized likelihood ratio test}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-4}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/Multi-Temp.2017.8035245}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/8035245}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{Multitemp2017.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2016</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="7729869" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">A decomposition model for scatterers change detection in multi-temporal series of SAR images</div>
          <!-- Author -->
          <div class="author">
                  <em>Lobry, Sylvain</em>,¬†Tupin, Florence,¬†and Denis, Lo√Øc
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)</em> 2016
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="/assets/pdf/IGARSS2016.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/7729869" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Library</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This paper presents a method for strong scatterers change detection in synthetic aperture radar (SAR) images based on a decomposition for multi-temporal series. The formulated decomposition model jointly estimates the background of the series and the scatterers. The decomposition model retrieves possible changes in scatterers and the date at which they occurred. An exact optimization method of the model is presented and applied to a TerraSAR-X time series.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">7729869</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lobry, Sylvain and Tupin, Florence and Denis, Lo√Øc}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A decomposition model for scatterers change detection in multi-temporal series of SAR images}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3362-3365}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/IGARSS.2016.7729869}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/abstract/document/7729869}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{IGARSS2016.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="7559390" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Non-Uniform Markov Random Fields for Classification of SAR Images</div>
          <!-- Author -->
          <div class="author">
                  <em>Lobry, Sylvain</em>,¬†Tupin, Florence,¬†and Fjortoft, Roger
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of EUSAR 2016: 11th European Conference on Synthetic Aperture Radar</em> 2016
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="/assets/pdf/EUSAR2016.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://ieeexplore.ieee.org/document/7559390" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Library</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>When dealing with SAR image classification, the class parameters may vary along the swath for several reasons. Traditional classification algorithms are then not well adapted, as they assume constant class parameters. In this paper, we propose a binary classification algorithm based on Markov Random Fields that take into account the parameters variations in the swath, and we present results obtained on airborne TropiSAR and simulated SWOT HR data.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">7559390</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lobry, Sylvain and Tupin, Florence and Fjortoft, Roger}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of EUSAR 2016: 11th European Conference on Synthetic Aperture Radar}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Non-Uniform Markov Random Fields for Classification of SAR Images}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-4}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/7559390}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{EUSAR2016.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">project</span> <span class="p">=</span> <span class="s">{SWOT}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2015</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="7245772" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Sparse + smooth decomposition models for multi-temporal SAR images</div>
          <!-- Author -->
          <div class="author">
                  <em>Lobry, Sylvain</em>,¬†Denis, Lo√Øc,¬†and Tupin, Florence
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In 2015 8th International Workshop on the Analysis of Multitemporal Remote Sensing Images (Multi-Temp)</em> 2015
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="/assets/pdf/Multitemp2015.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://ieeexplore.ieee.org/document/7245772" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Library</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>SAR images have distinctive characteristics compared to optical images: speckle phenomenon produces strong fluctuations, and strong scatterers have radar signatures several orders of magnitude larger than others. We propose to use an image decomposition approach to account for these peculiarities. Several methods have been proposed in the field of image processing to decompose an image into components of different nature, such as a geometrical part and a textural part. They are generally stated as an energy minimization problem where specific penalty terms are applied to each component of the sought decomposition. We decompose temporal series of SAR images into three components: speckle, strong scatterers and background. Our decomposition method is based on a discrete optimization technique by graph-cut. We apply it to change detection tasks.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">7245772</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lobry, Sylvain and Denis, Lo√Øc and Tupin, Florence}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2015 8th International Workshop on the Analysis of Multitemporal Remote Sensing Images (Multi-Temp)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Sparse + smooth decomposition models for multi-temporal SAR images}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-4}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/Multi-Temp.2015.7245772}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://ieeexplore.ieee.org/document/7245772}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{Multitemp2015.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>


<br><br>
<div class="publications">
<h2>National Conferences</h2>
  <h2 class="year">2023</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="Montginoux2023GRETSI" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">√âvaluation du couvert neigeux √† partir d‚Äôimages SAR par apprentissage profond bas√© sur des images optiques de r√©f√©rence (to appear)</div>
          <!-- Author -->
          <div class="author">Montginoux, Mathias,¬†Weissgerber, Flora,¬†
                  <em>Lobry, Sylvain</em>,¬†and Idier, J√©r√¥me
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In GRETSI</em> 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Optical satellite images are commonly used to evaluate the snow cover. However, part of the information is lost due to clouds. 
To fill this gap we propose to detect the snow from Sentinel-1 SAR images using a convolutional neural network trained with labels obtained from MODIS optical images. A binary semantic segmentation is computed from two polarimetric SAR inputs: a wet snow ratio and a dry snow ratio.
The model, called SESAR U-net, is trained on a small area and then tested over a whole watershed. The missing labels are interpolated and the uncertainty due to clouds is considered. Our proposed method gives an overall accuracy higher than 80%. </p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Montginoux2023GRETSI</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{√âvaluation du couvert neigeux √† partir d'images SAR par apprentissage profond bas√© sur des images optiques de r√©f√©rence (to appear)}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Montginoux, Mathias and Weissgerber, Flora and Lobry, Sylvain and Idier, J√©r√¥me}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{GRETSI}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="Leygonie2023ORASIS" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Transposition de donn√©es mutlidimensionelles en images pour pallier le fl√©au de la dimension</div>
          <!-- Author -->
          <div class="author">Leygonie, Rebecca,¬†
                  <em>Lobry, Sylvain</em>,¬†Vimont, Guillaume,¬†and Wendling, Laurent
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In ORASIS</em> 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>When dealing with high-dimensional multivariate time series classification problems, a well-known difficulty is the \textitcurse of dimensionality. 
In this article, we propose an original approach of transposition of multidimensional data into images to tackle the task of classification. We propose a small hybrid model containing convolutional layers as a feature extractor followed by a recurrent neural network that take this transposed data as an input. We apply our method to a large dataset consisting of individual patient medical records. We show that our approach allows us to significantly reduce the size of a network and increase its performance by opting for a transformation of the input data.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Leygonie2023ORASIS</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Transposition de donn√©es mutlidimensionelles en images pour pallier le fl√©au de la dimension}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Leygonie, Rebecca and Lobry, Sylvain and Vimont, Guillaume and Wendling, Laurent}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ORASIS}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="Leygonie2022RFIAP" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Apprentissage profond pour la classification de QR Codes bruit√©s</div>
          <!-- Author -->
          <div class="author">Leygonie, Rebecca,¬†
                  <em>Lobry, Sylvain</em>,¬†and Wendling, Laurent
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In RFIAP/CAP 2022</em> 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We wish to define the limitations of a classical classification model based on deep learning when applied on abstract images, which do not represent visually identifiable objects.
QR Codes fall into this category of abstract images: one bit corresponding to one encoded character, QR codes were not designed to be decoded by the naked eye. To understand the limitations of a deep learning-based model for abstract image classification, we train an image classification model on QR codes generated from the information obtained when reading a health pass. We compare the performance of a classification model with that of a classical (deterministic) decoding method in the presence of noise. This study allows us to conclude that a model based on deep learning can be relevant for the understanding of abstract images.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Leygonie2022RFIAP</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Apprentissage profond pour la classification de QR Codes bruit√©s}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Leygonie, Rebecca and Lobry, Sylvain and Wendling, Laurent}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{RFIAP/CAP 2022}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="letheule2021segmentation" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Segmentation S√©mantique pour la Simulation d‚ÄôImages SAR</div>
          <!-- Author -->
          <div class="author">Letheule, Nathan,¬†Weissgerber, Flora,¬†
                  <em>Lobry, Sylvain</em>,¬†and Colin-Koeniguer, Elise
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In ORASIS 2021</em> 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="https://hal.archives-ouvertes.fr/hal-03339666/document" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Simulation of Synthetic Aperture Radar (SAR) images is an essential component of SAR applications development. This can be done using style transfer methods or through physical simulators. We propose a hybrid approach : physical simulation of a SAR image from a material map ob- tained by a deep network taking the optical image as input. We compare the simulations with those from a style transfer method. The first results show the potential of our approach.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">letheule2021segmentation</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Segmentation S{\'e}mantique pour la Simulation d'Images SAR}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Letheule, Nathan and Weissgerber, Flora and Lobry, Sylvain and Colin-Koeniguer, Elise}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ORASIS 2021}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{https://hal.archives-ouvertes.fr/hal-03339666/document}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2017</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="lobry2017detection" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">D√©tection de l‚Äôeau dans les images radar du futur satellite SWOT</div>
          <!-- Author -->
          <div class="author">
                  <em>Lobry, Sylvain</em>,¬†Fjortoft, Roger,¬†Denis, Loƒ±Ãàc,¬†and Tupin, Florence
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In GRETSI</em> 2017
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="/assets/pdf/GRETSI2017.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>One of the objectives of the SWOT mission conducted by CNES and JPL is to obtain a global measurement of water heights. In order to apply an interferometric processing on SWOT images over continents, a first step is to obtain a classification indicating the presence of water. We introduce two methods adapted to the unusual acquisition parameters of the sensor for the detection of compact areas (i.e. lakes) and linear networks (i.e. rivers).</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lobry2017detection</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{D{\'e}tection de l‚Äôeau dans les images radar du futur satellite SWOT}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lobry, Sylvain and Fjortoft, Roger and Denis, Lo{\"\i}c and Tupin, Florence}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{GRETSI}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{GRETSI2017.pdf}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
  <span class="na">project</span> <span class="p">=</span> <span class="s">{SWOT}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2016</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        

        <!-- Entry bib key -->
        <div id="lobry2016modele" class="col-sm-10">
        
          <!-- Title -->
          <div class="title">Un mod√®le de d√©composition pour la d√©tection de changement dans les s√©ries temporelles d‚Äôimages RSO</div>
          <!-- Author -->
          <div class="author">
                  <em>Lobry, Sylvain</em>,¬†Denis, Loƒ±Ãàc,¬†and Tupin, Florence
          </div>

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In RFIA</em> 2016
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            <a href="/assets/pdf/RFIA2016" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a class="bibtex btn btn-sm z-depth-0" role="button">BibTex</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This paper presents a method for strong scatterers change detection in synthetic aperture radar (SAR) images based on a decomposition for multi-temporal series. The formula- ted decomposition model jointly estimates the background of the series and the scatterers. The decomposition mo- del retrieves possible changes in scatterers and the date at which they occurred. An exact optimization method of the model is presented and applied to a TerraSAR-X time series.</p>
          </div>
<!-- Hidden bibtex block -->
          <div class="bibtex hidden">
            <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lobry2016modele</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Un mod{\`e}le de d{\'e}composition pour la d{\'e}tection de changement dans les s{\'e}ries temporelles d‚Äôimages RSO}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lobry, Sylvain and Denis, Lo{\"\i}c and Tupin, Florence}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{RFIA}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">pdf</span> <span class="p">=</span> <span class="s">{RFIA2016}</span><span class="p">,</span>
  <span class="na">bibtex_show</span> <span class="p">=</span> <span class="s">{true}</span>
<span class="p">}</span></code></pre></figure>
          </div>
        </div>
      </div>
</li></ol>


</div>
</div>

          </article>

        </div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        ¬© Copyright 2023 Sylvain  Lobry. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Mansory & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/mansory.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>

